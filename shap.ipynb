{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(input_size),\n\u001b[1;32m     16\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(input_size),\n\u001b[1;32m     17\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     18\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[1;32m     19\u001b[0m ])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Preprocess the image tensor\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/faizahkureshi/Desktop/Capstone Project/dataset/test/Elephant/ele.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Convert the preprocessed image tensor to a tensor that can be used as input to the model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tensor_image \u001b[38;5;241m=\u001b[39m preprocessed_image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=93'>94</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=94'>95</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1508'>1509</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1509'>1510</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1510'>1511</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1514'>1515</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1518'>1519</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1519'>1520</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=268'>269</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=269'>270</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=270'>271</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=271'>272</a>\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=274'>275</a>\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=275'>276</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=276'>277</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py:349\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=345'>346</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=346'>347</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=348'>349</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:926\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py?line=923'>924</a>\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py?line=924'>925</a>\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py?line=925'>926</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import shap\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "model = vgg16(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "input_size = 112\n",
    "\n",
    "# Create a transform that resizes the image to the required input size and normalizes the pixel values\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Preprocess the image tensor\n",
    "preprocessed_image = preprocess(Image.open(\"/Users/faizahkureshi/Desktop/Capstone Project/dataset/test/Elephant/ele.png\"))\n",
    "\n",
    "# Convert the preprocessed image tensor to a tensor that can be used as input to the model\n",
    "tensor_image = preprocessed_image.unsqueeze(0)\n",
    "\n",
    "# Compute SHAP values using the tensor_image tensor\n",
    "shap_values = explainer.shap_values(tensor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    if image.max() > 1:\n",
    "        image /= 255\n",
    "    image = (image - mean) / std\n",
    "    # in addition, roll the axis so that they suit pytorch\n",
    "    return torch.tensor(image.swapaxes(-1, 1).swapaxes(2, 3)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "X, y = shap.datasets.imagenet50()\n",
    "\n",
    "X /= 255\n",
    "\n",
    "to_explain = X[[40, 41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that because the inputs are scaled to be between 0 and 1, the local smoothing also has to be\n",
    "# scaled compared to the Keras model\n",
    "explainer = shap.GradientExplainer(\n",
    "    (model, model.features[7]), normalize(X), local_smoothing=0.5\n",
    ")\n",
    "shap_values, indexes = explainer.shap_values(\n",
    "    normalize(to_explain), ranked_outputs=2, nsamples=10\n",
    ")\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shap_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of SHAP values array: [(128, 112, 2, 112), (128, 112, 2, 112)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of SHAP values array:\", [s.shape for s in shap_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data: (2, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of input data:\", to_explain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beer_bottle' 'beer_glass']\n",
      " ['meerkat' 'mongoose']]\n"
     ]
    }
   ],
   "source": [
    "index_names = index_names.reshape((len(to_explain), -1))\n",
    "print(index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/faizahkureshi/Desktop/Capstone Project/dataset/test/Elephant/ele.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[0;32m---> 33\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Ensure the input tensor has the correct shape (1, 3, height, width)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m preprocessed_image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=93'>94</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=94'>95</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1508'>1509</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1509'>1510</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1510'>1511</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1514'>1515</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1518'>1519</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1519'>1520</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=268'>269</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=269'>270</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=270'>271</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=271'>272</a>\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=274'>275</a>\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=275'>276</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/transforms.py?line=276'>277</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py:349\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=345'>346</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=346'>347</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/functional.py?line=348'>349</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:926\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py?line=923'>924</a>\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py?line=924'>925</a>\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/faizahkureshi/anaconda3/envs/testenv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py?line=925'>926</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shap\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Load the model\n",
    "model = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "# Load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# Load and preprocess the input image\n",
    "input_size = 224\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# Load and preprocess the image tensor\n",
    "image_path = \"/Users/faizahkureshi/Desktop/Capstone Project/dataset/test/Elephant/ele.png\"\n",
    "image = Image.open(image_path)\n",
    "preprocessed_image = preprocess(image)\n",
    "\n",
    "# Ensure the input tensor has the correct shape (1, 3, height, width)\n",
    "preprocessed_image = preprocessed_image.unsqueeze(0)\n",
    "\n",
    "# Get SHAP values\n",
    "explainer = shap.GradientExplainer((model, model.features[7]), torch.zeros(1, 3, input_size, input_size))\n",
    "shap_values, indexes = explainer.shap_values(preprocessed_image, ranked_outputs=2, nsamples=10)\n",
    "\n",
    "# Rescale SHAP values to match input image dimensions\n",
    "shap_values = [s[0].detach().cpu().numpy() for s in shap_values]\n",
    "shap_values = [s.transpose(1, 2, 0) for s in shap_values]\n",
    "\n",
    "# Plot SHAP values overlayed on the input image\n",
    "plt.figure()\n",
    "plt.imshow(preprocessed_image.squeeze(0).permute(1, 2, 0))  # Convert tensor to numpy array and permute dimensions\n",
    "for i in range(len(shap_values)):\n",
    "    plt.imshow(shap_values[i].sum(axis=-1), cmap='jet', alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fa14ea9922aa85af2e126d7713a70e31456fae90c96900f0e8fcf2763b2bd21"
  },
  "kernelspec": {
   "display_name": "Python 3.10.13 ('testenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
